# Disaster Recovery Plan — Bybit Algo-Grid / AVI-5

Документ описывает порядок действий при авариях и потере работоспособности
системы **Bybit Algo-Grid / стратегия AVI-5**.

Цель DR-плана — обеспечить:

- **RTO (Recovery Time Objective)** = **15 минут**  
  Максимально допустимое время восстановления работоспособности критичного функционала.
- **RPO (Recovery Point Objective)** = **5 минут**  
  Максимально допустимая потеря данных по времени (от последней точки восстановления).

Эти значения согласованы с требованиями к резервированию и backup, описанными в
`docs/backup_strategy.md` и соответствующем разделе архитектурного документа.

---

## 1. Область действия и предпосылки

DR-план распространяется на:

- backend AVI-5 (FastAPI-приложение);
- базу данных PostgreSQL (основное хранилище сигналов, позиций, пользователей и настроек);
- Redis (кэш и вспомогательные структуры);
- интеграцию с биржей Bybit (WS и REST);
- инфраструктурный стек (Docker/Kubernetes, Traefik/Ingress, Prometheus/Grafana).

Предполагается наличие:

- регулярных резервных копий БД согласно `docs/backup_strategy.md`;
- мониторинга и алертинга (`monitoring/alerts.yml`);
- health-эндпоинта `/health` и метрик на `/metrics`;
- установленного и задокументированного механизма kill-switch.

---

## 2. Цели по доступности: RTO и RPO

### 2.1. RTO = 15 минут

- После критического инцидента (см. раздел 3) система должна быть
  переведена в ограниченно работоспособное состояние (как минимум: доступ к позициям,
  возможность включить/выключить торговлю, корректные /health и /metrics)
  **не позднее чем через 15 минут** с момента начала инцидента.

### 2.2. RPO = 5 минут

- Потеря данных за последние **до 5 минут** считается приемлемой.
- Для достижения RPO используются:
  - регулярные полные бэкапы БД;
  - WAL-архивирование и point-in-time recovery (см. `docs/backup_strategy.md`);
  - восстанавливающий скрипт `scripts/restore_db.sh <backup_timestamp>`.

---

## 3. Классы инцидентов

### 3.1. Класс A — Отказ БД (PostgreSQL)

Примеры:

- полная недоступность БД;
- серьёзная коррупция данных/таблиц;
- потеря основного инстанса без быстрой автоматической реплики.

Последствия:

- невозможность выполнять торговые операции и работать с историей;
- потенциальная частичная потеря данных (ограниченная RPO).

---

### 3.2. Класс B — Отказ узла приложения

Примеры:

- падение контейнера/подов приложения;
- цикл перезапуска / CrashLoopBackOff;
- деградация по CPU/RAM, приводящая к таймаутам.

Последствия:

- пользователи/внешние системы не могут обращаться к API;
- сигналы и события не обрабатываются, но данные в БД остаются в консистентном состоянии.

---

### 3.3. Класс C — Проблемы на стороне биржи (Bybit) или внешних сервисов

Примеры:

- длительные таймауты REST-запросов к Bybit;
- отсутствие или сильные задержки данных в WebSocket-потоке;
- сильное расхождение между локальным и биржевым состоянием позиций;
- проблемы с Vault/шифрованием секретов, затрагивающие интеграцию.

Последствия:

- невозможность/ограничение выставления новых ордеров;
- потенциальные расхождения по состоянию позиций;
- риск неконтролируемого поведения без kill-switch.

---

## 4. Процедура восстановления БД из бэкапа

Цель: восстановить работоспособную БД с потерей данных не более, чем на интервал RPO.

### 4.1. Подготовка

1. **Зафиксировать инцидент**:
   - время начала;
   - наблюдаемые симптомы;
   - принятые временные меры (включение kill-switch и т.п.).
2. **Перевести систему в безопасный режим**:
   - включить kill-switch (отключить новые торговые операции);
   - запретить любые миграции/изменения схемы до конца восстановления.
3. **Определить целевую точку восстановления**:
   - ориентир — момент времени за ≤ 5 минут до начала инцидента;
   - зафиксировать её в UTC (например, `2025-01-10T12:40:00Z`).

### 4.2. Восстановление БД

> Подробная техническая реализация описана в `docs/backup_strategy.md`. Здесь — операционный runbook.

1. На DR- или восстановительном хосте (либо на основном, если другой недоступен):
   - убедиться, что PostgreSQL остановлен (нет активных процессов, порт 5432 свободен);
   - убедиться, что доступен storage с бэкапами (локальный/S3/MinIO).
2. Выполнить восстановление:

```bash
   ./scripts/restore_db.sh <backup_timestamp>
```

где `<backup_timestamp>` — целевая точка восстановления (см. 4.1.3).
3. Дождаться завершения скрипта и проверить:

* успешный статус выполнения;
* отсутствие критичных ошибок в логах восстановления.

### 4.3. Проверка целостности данных

1. Запустить временный клиент (psql или любой DB GUI) к восстановленной БД.
2. Проверить:

   * наличие ключевых таблиц (`users`, `signals`, `positions`, `api_keys` и т.п.);
   * количество строк в ключевых таблицах примерно соответствует ожидаемому;
   * последние записи по времени не позже RPO (не более 5 минут потери).
3. При наличии автотестов для схемы БД — запустить smoke-тесты против восстановленной БД.

### 4.4. Переключение приложения на восстановленную БД

1. Обновить `DATABASE_URL` в:

   * переменных окружения для app (docker-compose / Kubernetes Secret);
   * CI/CD, если требуется.
2. Перезапустить приложение (см. раздел 5).
3. Убедиться, что:

   * `/health` возвращает успешный статус;
   * `/metrics` доступны и показывают ожидаемые значения.
4. Запустить **reconciliation** (служба сверки состояния) для проверки консистентности позиций/сигналов с биржей.

---

## 5. Процедура восстановления узла приложения

Цель: вернуть доступность API и обработку сигналов при сохранных данных в БД.

### 5.1. Общие шаги

1. Зафиксировать инцидент (время, симптомы, скриншоты/логи).
2. Проверить состояние зависимостей:

   * БД (подключение, простейший SELECT);
   * Redis (PING);
   * Bybit (по возможностям, либо через отдельные health-check скрипты).
3. При необходимости включить kill-switch, чтобы остановить новые торговые операции до стабилизации.

---

### 5.2. Восстановление в окружении Docker-compose

1. На хосте со стэком:

   ```bash
   docker compose ps
   ```

   Убедиться, что сервис `app` находится в состоянии `exited` / в цикле рестартов / отсутствует.
2. Перезапустить приложение:

   ```bash
   docker compose up -d app
   ```

   либо пересобрать и задеплоить новую версию образа через CI/CD.
3. Проверить логи:

   ```bash
   docker compose logs -f app
   ```

   * нет ли ошибок подключения к БД/Redis;
   * нет ли необработанных исключений на старте.
4. Проверить `/health` и `/metrics`:

   * `/health` — должен вернуть `status: "ok"` или аналогичный успешный статус;
   * `/metrics` — должен отдавать метрики Prometheus без ошибок.
5. При наличии нескольких инстансов (scale):

   ```bash
   docker compose up -d --scale app=2
   ```

   Убедиться, что все инстансы прошли health-check.

---

### 5.3. Восстановление в Kubernetes

1. Проверить состояние pod’ов:

   ```bash
   kubectl get pods -n trading -l app=avi5-backend
   ```

2. При необходимости инициировать перезапуск:

   ```bash
   kubectl rollout restart deployment/avi5-backend -n trading
   ```

3. Отследить rollout:

   ```bash
   kubectl rollout status deployment/avi5-backend -n trading
   ```

   Убедиться, что статус `successfully rolled out`.

4. Проверить readiness и liveness:

   * для каждого pod’а состояние должно быть `READY 1/1`;
   * `kubectl describe pod` не должен содержать частых рестартов или OOMKilled.

5. Проверить `/health` и `/metrics` через сервис `avi5-backend` или через Ingress.

---

## 6. Инциденты на стороне биржи (Bybit) и внешних сервисов

Цель: предотвратить неконтролируемое поведение при проблемах у внешних провайдеров.

### 6.1. Симптомы

* резкое увеличение error-rate по Bybit REST/WS в метриках (`monitoring/alerts.yml`);
* отсутствие обновлений по WS (нет новых сигналов/цитат);
* значимое расхождение между локальными и биржевыми позициями по результатам reconciliation.

### 6.2. Действия при серьёзном инциденте

1. Проверить статус биржи (status page Bybit, официальный канал связи).
2. При подтверждённой проблеме:

   * **включить kill-switch**, чтобы остановить новые торговые операции;
   * перевести UI в режим read-only (если реализовано).
3. Продолжать сбор метрик и логирование, не предпринимать попытки агрессивного ретрая,
   чтобы не усугублять ситуацию (anti-churn).

### 6.3. Восстановление после инцидента

1. После подтверждения нормализации работы биржи:

   * проверить базовый REST/WS-запрос вручную;
   * убедиться, что новые котировки приходят стабильно.
2. Запустить полную **сверку состояния** (reconciliation):

   * сравнить локальные позиции с биржей;
   * для расхождений — либо исправить вручную, либо использовать предусмотренные автоматизированные сценарии.
3. При успешной сверке и отсутствии аномалий:

   * в координации с владельцем системы снять kill-switch;
   * восстановить обычный режим торговли.
4. Зафиксировать инцидент и выводы (post-mortem).

---

## 7. Регулярные DR-учения и проверки

Цель: убедиться, что описанные процедуры реально работают и укладываются в целевые RTO/RPO.

### 7.1. Периодичность

* **Не реже одного раза в квартал** проводится формальное DR-учение,
  имитирующее один или несколько классов инцидентов:

  * отказ БД;
  * отказ узла приложения;
  * проблемы на стороне биржи (по возможности — на стэйдже/сандбоксе).

### 7.2. Сценарий DR-учений (пример)

1. Выбор сценария (например, «потеря основного инстанса БД»).
2. Подготовка стенда:

   * отдельное окружение (staging) с копией prod-конфигурации;
   * копия реальных бэкапов (обезличенных, при необходимости).
3. Инициирование «аварии»:

   * искусственная остановка БД или уничтожение тестового инстанса;
   * фиксируется время начала (T0).
4. Полное следование процедуре разделов 4–5:

   * восстановление БД из бэкапа;
   * перезапуск приложения;
   * проверки `/health`, `/metrics` и reconciliation.
5. Замер фактических:

   * RTO — время от T0 до восстановления основного функционала;
   * RPO — разница между последними данными до «аварии» и после восстановления.
6. Оформление отчёта:

   * удалось ли уложиться в целевые RTO/RPO;
   * обнаруженные проблемы и bottleneck’и;
   * перечень необходимых улучшений в процессе или инструментах.

### 7.3. Связь с backup-стратегией

* Результаты DR-учений должны подтверждать:

  * что бэкапы из `docs/backup_strategy.md` **реально восстанавливаемы**;
  * что off-site хранение и WAL-архивирование обеспечивают заявленный RPO;
  * что скрипты (`scripts/restore_db.sh` и прочие утилиты) актуальны и протестированы.
* При выявлении проблем DR-процедуры и backup-стратегия обновляются синхронно.

---

## 8. Ответственность и актуализация документа

* Ответственность за исполнение DR-плана несёт выделенная ops/SRE-команда (или ответственное лицо со стороны Заказчика, если это зафиксировано контрактом).
* Документ `docs/disaster_recovery.md` должен пересматриваться:

  * при изменении архитектуры;
  * при изменении backup-стратегии;
  * по итогам каждого крупного инцидента или DR-учения.

Любые изменения в этом документе согласуются с владельцем системы и, при необходимости, с юридическим отделом.
