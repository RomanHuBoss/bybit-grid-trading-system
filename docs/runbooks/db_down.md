<!-- docs/runbooks/db_down.md -->

# Runbook: Недоступность БД (`db_down`)

## 1. Контекст

Алерт `db_down` означает, что основной экземпляр PostgreSQL недоступен для приложения.  
Это критичный инцидент: торговля и API фактически остановлены.

## 2. Как распознать проблему

1. `/health` возвращает `db=down` или HTTP 5xx.
2. Алерт `db_down` из `monitoring/alerts.yml`.
3. В логах приложения — ошибки подключения к БД (`ConnectionError`, `DatabaseError`).

## 3. Пошаговые действия (DR-runbook кратко)

1. Зафиксировать время начала инцидента.
2. Проверить:
   - состояние инстанса БД (managed-DB консоль / `systemctl status` / `kubectl` — в зависимости от развёртывания);
   - наличие инцидентов у облачного провайдера (если DB managed).

### 3.1. Если инстанс жив, но не отвечает

1. Проверить:
   - свободное место на диске;
   - состояние процессов/Postgres-логов.
2. При необходимости:
   - мягко перезапустить PostgreSQL;
   - убедиться, что он поднялся без recovery-циклов.

### 3.2. Если инстанс реально упал / потерян

Дальнейшие шаги — согласно `docs/disaster_recovery.md`:

1. Принять решение:
   - переключение на реплику/standby (если есть);
   - либо восстановление из бэкапа через `scripts/restore_db.sh`.
2. При переключении на реплику:
   - обновить `DATABASE_URL` в конфиге приложения;
   - перевести реплику в `primary` (с учётом используемого механизма репликации).
3. При восстановлении из бэкапа:
   - выбрать точку восстановления с учётом RPO (≤ 5 минут потери);
   - выполнить `scripts/restore_db.sh <backup_timestamp>`;
   - проверить целостность данных.

## 4. Проверка восстановления

1. `/health` показывает `db=ok`.
2. Простейшие SELECT-запросы проходят, ключевые таблицы доступны.
3. На Grafana-дашборде нет продолжения всплеска 5xx от API.

## 5. Эскалация

- Инцидент уровня SEV-1/SEV-2 в зависимости от длительности и влияния.
- Обязателен post-mortem:
  - корневая причина;
  - нужны ли изменения в DR/backup-стратегии;
  - нужны ли дополнительные алерты (например, раннее предупреждение по диску/репликации).
